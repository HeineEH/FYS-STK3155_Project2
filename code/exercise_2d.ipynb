{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport utils.neural_network, utils.training, utils.step_methods, utils.cost_functions, utils.activation_functions, utils.utils\n",
    "\n",
    "# Imports\n",
    "from utils.neural_network import NeuralNetwork\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.training import TrainingMethod, StochasticGradientDescent, GradientDescent\n",
    "from utils.step_methods import StepMethod, RMSpropStep, AdamStep\n",
    "from utils.cost_functions import MSE\n",
    "from utils.activation_functions import Sigmoid, Identity, ReLU, LeakyReLU\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from utils.utils import generate_dataset, runge, plot_mse_data, analyze_model_learning_rates\n",
    "from copy import deepcopy\n",
    "\n",
    "plt.style.use('./utils/_plot_style.mplstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "np.random.seed(124)\n",
    "\n",
    "n = 500\n",
    "x, y = generate_dataset(n)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2,random_state=44)\n",
    "\n",
    "# Scale dataset\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "y_mean = y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS model\n",
    "ols_model = make_pipeline(\n",
    "    PolynomialFeatures(degree=12, include_bias=False),\n",
    "    StandardScaler(),\n",
    "    LinearRegression(fit_intercept=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "\n",
    "nodes = list(range(6,22,2))\n",
    "hidden_layers = [1,2,3]\n",
    "\n",
    "activation_funcs = [\n",
    "    Sigmoid(),\n",
    "    ReLU(), \n",
    "    LeakyReLU()\n",
    "]\n",
    "\n",
    "training_method = (\"SGD\", StochasticGradientDescent, (-4, 0))\n",
    "step_method = (\"ADAM\", AdamStep(0, 0.9, 0.999))\n",
    "\n",
    "'''\n",
    "# Define training methods\n",
    "training_methods: list[tuple[str, type[TrainingMethod], tuple[int, int]]] = [\n",
    "    (\"SGD\", StochasticGradientDescent, (-4, 0)),\n",
    "    (\"GD\", GradientDescent, (-6, -2)),\n",
    "]\n",
    "\n",
    "# Step methods\n",
    "step_methods: list[tuple[str, StepMethod]] = [\n",
    "    (\"RMSprop\", RMSpropStep(0, 0.9)), \n",
    "    (\"ADAM\", AdamStep(0, 0.9, 0.999))\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_learning_rates = 9\n",
    "iterations = 3000\n",
    "all_mse_data = np.zeros((len(nodes), len(hidden_layers), len(activation_funcs),n_learning_rates))\n",
    "\n",
    "# Analyze mse vs. learning rates for each combination of model, training method, and step method\n",
    "for i, n_nodes in enumerate(nodes):\n",
    "    print(model_title)\n",
    "    for j, (training_method_name, training_method, (min_log_lr, max_log_lr)) in enumerate(training_methods):\n",
    "        print(f\"\\t{training_method_name} - LR range: [{10**min_log_lr:.1e}, {10**max_log_lr:.1e}]\")\n",
    "        for k, (step_method_name, step_method) in enumerate(step_methods):\n",
    "            print(f\"\\t\\t{step_method_name}\", end=\"\")\n",
    "\n",
    "            learning_rates = list(np.logspace(min_log_lr, max_log_lr, n_learning_rates))\n",
    "\n",
    "            np.random.seed(124)\n",
    "            mse_data = analyze_model_learning_rates(\n",
    "                model,\n",
    "                training_method(\n",
    "                    step_method = step_method,\n",
    "                    inputs = x_train_scaled,\n",
    "                    targets = y_train - y_mean,\n",
    "                    test_inputs = x_test_scaled,\n",
    "                    test_targets = y_test - y_mean,\n",
    "                ),\n",
    "                learning_rates, \n",
    "                iterations,\n",
    "            )\n",
    "            print(\".\")\n",
    "            all_mse_data[i][j][k] = mse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSE vs. learning rates for each model\n",
    "\n",
    "fig, axes = plt.subplots(1, len(models), figsize=(12, 4), sharey=True)\n",
    "for i, (model_title, _) in enumerate(models):\n",
    "    for j, (training_method_name, _, (min_log_lr, max_log_lr)) in enumerate(training_methods):\n",
    "        learning_rates = list(np.logspace(min_log_lr, max_log_lr, n_learning_rates))\n",
    "        for k, (step_method_name, _) in enumerate(step_methods):\n",
    "            mse_data = all_mse_data[i][j][k]\n",
    "            axes[i].plot(learning_rates, mse_data, \n",
    "                     label=f\"{training_method_name} with {step_method_name}\", \n",
    "                     c=f\"C{k}\", \n",
    "                     linestyle = \"-\" if j == 0 else \"--\",\n",
    "            )\n",
    "    axes[i].set_xscale(\"log\")\n",
    "    axes[i].set_yscale(\"log\")\n",
    "    axes[i].set_xlabel(\"Learning rate\")\n",
    "axes[0].set_ylabel(\"MSE\")\n",
    "single_legends = axes[0].get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    handles=single_legends[0], labels=single_legends[1], \n",
    "    loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "    ncol=2, fancybox=True, shadow=True\n",
    ")\n",
    "plt.savefig(\"../figs/mse_vs_lr.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mse_data = np.zeros((len(models), len(training_methods), len(step_methods),iterations,1))\n",
    "\n",
    "# Analyze mse vs. learning rates for each combination of model, training method, and step method\n",
    "for i, (model_title, model) in enumerate(models):\n",
    "    print(model_title)\n",
    "    for j, (training_method_name, training_method, (min_log_lr, max_log_lr)) in enumerate(training_methods):\n",
    "        print(f\"\\t{training_method_name} - LR range: [{10**min_log_lr:.1e}, {10**max_log_lr:.1e}]\")\n",
    "        for k, (step_method_name, step_method) in enumerate(step_methods):\n",
    "            print(f\"\\t\\t{step_method_name}\", end=\"\")\n",
    "\n",
    "            learning_rates = list(np.log(max_log_lr))\n",
    "\n",
    "            np.random.seed(124)\n",
    "            mse_data = analyze_model_learning_rates(\n",
    "                model,\n",
    "                training_method(\n",
    "                    step_method = step_method,\n",
    "                    inputs = x_train_scaled,\n",
    "                    targets = y_train - y_mean,\n",
    "                    test_inputs = x_test_scaled,\n",
    "                    test_targets = y_test - y_mean,\n",
    "                ),\n",
    "                learning_rates, \n",
    "                iterations,\n",
    "                track_mse = True\n",
    "            )\n",
    "            print(\".\")\n",
    "            all_mse_data[i][j][k] = mse_data\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, len(models), figsize=(12, 4), sharey=True)\n",
    "for i, (model_title, _) in enumerate(models):\n",
    "    for j, (training_method_name, _, (min_log_lr, max_log_lr)) in enumerate(training_methods):\n",
    "        learning_rate = max_log_lr\n",
    "        for k, (step_method_name, _) in enumerate(step_methods):\n",
    "            mse_data = all_mse_data[i][j][k][:][0]\n",
    "            axes[i].plot(range(iterations), mse_data, \n",
    "                     label=f\"{training_method_name} with {step_method_name}\", \n",
    "                     c=f\"C{k}\", \n",
    "                     linestyle = \"-\" if j == 0 else \"--\",\n",
    "            )\n",
    "    axes[i].set_xscale(\"log\")\n",
    "    axes[i].set_yscale(\"log\")\n",
    "    axes[i].set_xlabel(\"Iterations\")\n",
    "axes[0].set_ylabel(\"MSE\")\n",
    "single_legends = axes[0].get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    handles=single_legends[0], labels=single_legends[1], \n",
    "    loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "    ncol=2, fancybox=True, shadow=True\n",
    ")\n",
    "plt.savefig(\"../figs/mse_vs_iterations_max_lr.pdf\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
