{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import NeuralNetwork\n",
    "import numpy as np\n",
    "from training import GradientDescent, StochasticGradientDescent\n",
    "from step_methods import ConstantLearningRateStep, MomentumStep, ADAgradStep, RMSpropStep, AdamStep\n",
    "\n",
    "# Defining some activation functions\n",
    "def ReLU(z):\n",
    "    return np.where(z > 0, z, 0)\n",
    "\n",
    "# Derivative of the ReLU function\n",
    "def ReLU_der(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def mse(predict, target):\n",
    "    return np.mean((predict - target) ** 2)\n",
    "\n",
    "def mse_der(predict, target):\n",
    "    n = target.size\n",
    "    return 2/n * (predict-target)\n",
    "\n",
    "def sigmoid_der(z):\n",
    "    sig = sigmoid(z)\n",
    "    return sig*(1-sig)\n",
    "\n",
    "def cross_entropy(predict, target):\n",
    "    return np.sum(-target * np.log(predict))\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"Compute softmax values for each set of scores in the rows of the matrix z.\n",
    "    Used with batched input data.\"\"\"\n",
    "    e_z = np.exp(z - np.max(z, axis=1,keepdims = True))\n",
    "    return e_z / np.sum(e_z, axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342.00808485934584\n",
      "131.24091325708102\n"
     ]
    }
   ],
   "source": [
    "from neural_network import NeuralNetwork\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "N = 100\n",
    "network_input_size = 4\n",
    "network_output_size = 3\n",
    "\n",
    "np.random.seed(123)\n",
    "inputs = np.random.rand(N, network_input_size) # (N, n_feat)\n",
    "targets = np.zeros((N,network_output_size))\n",
    "for i in range(N): \n",
    "    idx = np.random.randint(0,network_output_size)\n",
    "    targets[i,idx] = 1\n",
    "\n",
    "layer_output_sizes = [10, 8, network_output_size]\n",
    "activation_funcs = [ReLU, ReLU, softmax]\n",
    "activation_ders = [ReLU_der, ReLU_der, lambda z: 1]\n",
    "softmax_crossentropy_der = lambda predict, target: (predict - target)\n",
    "\n",
    "net = NeuralNetwork(network_input_size,\n",
    "        layer_output_sizes,\n",
    "        activation_funcs,\n",
    "        activation_ders,\n",
    "        cross_entropy,\n",
    "        softmax_crossentropy_der)\n",
    "\n",
    "print(net.cost_batch(inputs,targets))\n",
    "\n",
    "net.train(GradientDescent(step_method=RMSpropStep(learning_rate=0.016, decay_rate=0.9),inputs = inputs, targets = targets),100)\n",
    "\n",
    "print(net.cost_batch(inputs,targets))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
