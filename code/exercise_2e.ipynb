{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport utils.neural_network, utils.training, utils.step_methods, utils.cost_functions, utils.activation_functions, utils.utils\n",
    "\n",
    "# Imports\n",
    "from utils.neural_network import NeuralNetwork\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.training import TrainingMethod, StochasticGradientDescent, GradientDescent\n",
    "from utils.step_methods import StepMethod, RMSpropStep, AdamStep\n",
    "from utils.cost_functions import MSE\n",
    "from utils.activation_functions import Sigmoid, Identity, ReLU, LeakyReLU\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from utils.utils import generate_dataset, runge, plot_mse_data, analyze_model_learning_rates\n",
    "from copy import deepcopy\n",
    "\n",
    "plt.style.use('./utils/_plot_style.mplstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "np.random.seed(124)\n",
    "\n",
    "n = 500\n",
    "x, y = generate_dataset(n)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2,random_state=44)\n",
    "\n",
    "# Scale dataset\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "y_mean = y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(range(4,41,9))\n",
    "hidden_layers = [2,4,6]\n",
    "\n",
    "activation_funcs = [\n",
    "    (\"Sigmoid\", Sigmoid()),\n",
    "    (\"ReLU\", ReLU()), \n",
    "    (\"LeakyReLU\",LeakyReLU())\n",
    "]\n",
    "\n",
    "regularization_types = [\n",
    "    \"L1\", \n",
    "    \"L2\"\n",
    "]\n",
    "\n",
    "training_methods = (\"SGD\", StochasticGradientDescent, (-4, 0))\n",
    "step_methods = (\"ADAM\", AdamStep(0, 0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_learning_rates = 9\n",
    "iterations = 3000\n",
    "lamb_vals = np.logspace(-3,3,7)\n",
    "all_mse_data = np.zeros((len(lamb_vals), len(regularization_types), n_learning_rates))\n",
    "n_layers = 4\n",
    "n_nodes = 22\n",
    "activation_func = ReLU()\n",
    "\n",
    "# Analyze mse vs. learning rate and number of nodes for different numbers of hidden layers and activation functions. \n",
    "for i, lamb in enumerate(lamb_vals):\n",
    "    for j,regularization in enumerate(regularization_types): \n",
    "        print(\"Lambda: \",lamb_vals)\n",
    "        _,training_method, (min_log_lr,max_log_lr) = training_methods\n",
    "\n",
    "        learning_rates = list(np.logspace(min_log_lr, max_log_lr, n_learning_rates))\n",
    "\n",
    "        model = NeuralNetwork(\n",
    "                network_input_size = 1,\n",
    "                layer_output_sizes = [n_nodes]*n_layers + [1],\n",
    "                activation_funcs = [activation_func]*n_layers + [Identity()],\n",
    "                cost_fun = MSE(regularization = regularization,lambd = lamb)\n",
    "                )\n",
    "\n",
    "        np.random.seed(124)\n",
    "        mse_data = analyze_model_learning_rates(\n",
    "            model,\n",
    "            training_method(\n",
    "                step_method = step_methods[1],\n",
    "                inputs = x_train_scaled,\n",
    "                targets = y_train - y_mean,\n",
    "                test_inputs = x_test_scaled,\n",
    "                test_targets = y_test - y_mean,\n",
    "            ),\n",
    "            learning_rates, \n",
    "            iterations,\n",
    "        )\n",
    "        print(\".\")\n",
    "        all_mse_data[i][j] = mse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSE vs. learning rates for each model\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, len(regularization_types), figsize=(20, 16), sharey=True)\n",
    "for i,regularization_type in enumerate(regularization_types):\n",
    "    mse_data = all_mse_data[:,i,:]\n",
    "    sns.heatmap(\n",
    "        np.log10(np.array(mse_data)),  \n",
    "        ax = axes[i],\n",
    "        fmt=\".2f\",   \n",
    "        annot=True,\n",
    "        cmap=\"viridis\", \n",
    "        xticklabels=np.log10(np.array(learning_rates)),\n",
    "        yticklabels=nodes,cbar_kws={\"label\": r\"$log_{10}$(MSE)\"}\n",
    "        )\n",
    "    axes[j,k].set_title(fr\"{n_hidden_layers}, {name}\")\n",
    "    axes[j,k]\n",
    "\n",
    "fig.supxlabel(r\"$log_{10}$(Learning rate)\")\n",
    "fig.supylabel(\"# of nodes\")\n",
    "plt.savefig(\"../figs/mse_vs_lr_nodes.pdf\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
